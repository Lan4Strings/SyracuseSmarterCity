{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark import sql\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml import feature,regression,Pipeline,evaluation,classification\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Row,SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "import math\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incompelete this part keeponlytopcrimes\n"
     ]
    }
   ],
   "source": [
    "#read crime data\n",
    "crime_df = spark.read.csv('temp_crime2017-2018.csv', header=True, inferSchema=True)\n",
    "\n",
    "#descriptive analyzing and ploting\n",
    "def descriptive_analyzing(crime_df):\n",
    "    try:\n",
    "        Code\n",
    "    except:\n",
    "        print(\"incompelete this part descriptive_analyzing\")\n",
    "    \n",
    "\n",
    "\n",
    "#keep only top crimes, combine different types\n",
    "def keeponlytopcrimes(crime_df):\n",
    "    try:\n",
    "        Code\n",
    "        return new_crime_df\n",
    "    except:\n",
    "        print(\"incompelete this part keeponlytopcrimes\")\n",
    "\n",
    "        \n",
    "topcrimes_df = keeponlytopcrimes(crime_df)\n",
    "\n",
    "\n",
    "#keep only useful colunms\n",
    "def keepusefulcol(topcrimes_df):\n",
    "    try:\n",
    "        new_crime_df = spark.read.csv('temp2_crime2017-2018.csv', header=True, inferSchema=True)\n",
    "        return new_crime_df\n",
    "    except:\n",
    "        print(\"incompelete this part keepusefulcol\")\n",
    "\n",
    "    \n",
    "new_crime_df = keepusefulcol(topcrimes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read census data\n",
    "census_df = spark.read.csv('census_data.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for crime data\n",
    "\n",
    "#####Create month, season, daytime\n",
    "def createfeatures(new_crime_df):\n",
    "    try:\n",
    "        #code\n",
    "        new_crimes = spark.read.csv('temp4_crime2017-2018.csv', header=True, inferSchema=True)\n",
    "        return new_crimes\n",
    "    except:\n",
    "        print(\"incompelete this part createfeatures\")\n",
    "\n",
    "new_crimes = createfeatures(1)#new_crime_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+---------+-----+---+---+---+---+---+---+---+---+---+---+---+---------+-------+------------+----------+--------------+------------+--------+-------+-------+\n",
      "|CTN|              DATE1|    DATE2|Month|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|TIMESTART|TIMEEND|CODE_DEFINED|index_code|TIMESTART_MINS|TIMEEND_MINS|Midnight|Morning|Daytime|\n",
      "+---+-------------------+---------+-----+---+---+---+---+---+---+---+---+---+---+---+---------+-------+------------+----------+--------------+------------+--------+-------+-------+\n",
      "|100|2017-09-11 04:00:00|9/11/2017|  Sep|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|     1600|   1810|       Theft|         1|           960|        1090|       0|      0|      1|\n",
      "+---+-------------------+---------+-----+---+---+---+---+---+---+---+---+---+---+---+---------+-------+------------+----------+--------------+------------+--------+-------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_crimes.select(\"*\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for census data\n",
    "\n",
    "######calculate the 7values\n",
    "#Tenure rate\n",
    "def tenurerate(census_df):\n",
    "    try:\n",
    "        code\n",
    "        return col_census\n",
    "    except:\n",
    "        print(\"incompelete this part tenurerate\")\n",
    "\n",
    "#for Occupancy rate\n",
    "def occupancyrate(census_df):\n",
    "    try:\n",
    "        code\n",
    "        return col_census\n",
    "    except:\n",
    "        print(\"incompelete this part occupancyrate\")\n",
    "        \n",
    "#for Low income rate\n",
    "def lowincomerate(census_df):\n",
    "    try:\n",
    "        code\n",
    "        return col_census\n",
    "    except:\n",
    "        print(\"incompelete this part lowincomerate\")\n",
    "        \n",
    "#for high income rate\n",
    "def highincomerate(census_df):\n",
    "    try:\n",
    "        code\n",
    "        return col_census\n",
    "    except:\n",
    "        print(\"incompelete this part highincomerate\")\n",
    "        \n",
    "#for cheap property rate\n",
    "def expensiveproperty(census_df):\n",
    "    try:\n",
    "        code\n",
    "        return col_census\n",
    "    except:\n",
    "        print(\"incompelete this part expensiveproperty\")\n",
    "        \n",
    "#for high education rate\n",
    "def highedurate(census_df):\n",
    "    try:\n",
    "        code\n",
    "        return col_census\n",
    "    except:\n",
    "        print(\"incompelete this part highedurate\")\n",
    "\n",
    "#keep only 7values\n",
    "def combinecols(census_df):\n",
    "    try:\n",
    "        '''\n",
    "        tenurerate(census_df).alias(\"Tenure_rate\"),\n",
    "        highedurate(census_df).alias(\"High_Edu_rate\"),\n",
    "        expensiveproperty(census_df).alias(\"Expensive_prop_rate\"),\n",
    "        occupancyrate(census_df).alias(\"Ocuup_rate\"),\n",
    "        lowincomerate(census_df).alias(\"Low_income_rate\"),\n",
    "        highincomerate(census_df).alias(\"High_income_rate\")\n",
    "        '''\n",
    "        new_census = spark.read.csv('temp_census_data.csv', header=True, inferSchema=True)        \n",
    "        return new_census\n",
    "    except:\n",
    "        print(\"incompelete this part combinecols\")\n",
    "\n",
    "new_census = combinecols(1)#census_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------+----------+------------+----------------------+-----------+--------------+---------------+----------------+-----------------------+-------------------+\n",
      "|CTN|            CT|           Long name|Population|Square miles|people per square mile|Tenure Rate|Occupancy Rate|Low Income Rate|High Income Rate|Expensive Property Rate|High Education Rate|\n",
      "+---+--------------+--------------------+----------+------------+----------------------+-----------+--------------+---------------+----------------+-----------------------+-------------------+\n",
      "|100|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "+---+--------------+--------------------+----------+------------+----------------------+-----------+--------------+---------------+----------------+-----------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_census.select(\"*\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+-----+---+---+---+---+---+---+---+---+---+---+---+---------+-------+------------+----------+--------------+------------+--------+-------+-------+--------------+--------------------+----------+------------+----------------------+-----------+--------------+---------------+----------------+-----------------------+-------------------+\n",
      "|CTN|              DATE1|     DATE2|Month|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|TIMESTART|TIMEEND|CODE_DEFINED|index_code|TIMESTART_MINS|TIMEEND_MINS|Midnight|Morning|Daytime|            CT|           Long name|Population|Square miles|people per square mile|Tenure Rate|Occupancy Rate|Low Income Rate|High Income Rate|Expensive Property Rate|High Education Rate|\n",
      "+---+-------------------+----------+-----+---+---+---+---+---+---+---+---+---+---+---+---------+-------+------------+----------+--------------+------------+--------+-------+-------+--------------+--------------------+----------+------------+----------------------+-----------+--------------+---------------+----------------+-----------------------+-------------------+\n",
      "|100|2017-09-11 04:00:00| 9/11/2017|  Sep|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|     1600|   1810|       Theft|         1|           960|        1090|       0|      0|      1|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2017-01-03 05:00:00|  1/3/2017|  Jan|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      830|   1415|       Theft|         1|           510|         855|       0|      0|      1|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2018-01-22 05:00:00| 1/22/2018|  Jan|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     2030|   1005|       Theft|         1|          1230|         605|       0|      0|      1|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2017-07-14 04:00:00| 7/14/2017|  Jul|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|     1715|   2015|       Theft|         1|          1035|        1215|       0|      0|      0|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2017-01-18 05:00:00| 1/18/2017|  Jan|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     2251|   2251|       Theft|         1|          1371|        1371|       0|      0|      0|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2017-04-22 04:00:00| 4/22/2017|  Apr|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|     1859|   1859|     ASSAULT|         4|          1139|        1139|       0|      0|      0|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2017-05-11 04:00:00| 5/11/2017|  May|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|     1500|   1530|       Theft|         1|           900|         930|       0|      0|      1|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2017-12-17 05:00:00|12/17/2017|  Dec|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|     1327|   1327|       Theft|         1|           807|         807|       0|      0|      1|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2018-03-29 04:00:00| 3/29/2018|  Mar|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|     1619|   1619|       Theft|         1|           979|         979|       0|      0|      1|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2018-04-19 04:00:00| 4/19/2018|  Apr|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|     1858|   2038|     ASSAULT|         4|          1138|        1238|       0|      0|      0|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "+---+-------------------+----------+-----+---+---+---+---+---+---+---+---+---+---+---+---------+-------+------------+----------+--------------+------------+--------+-------+-------+--------------+--------------------+----------+------------+----------------------+-----------+--------------+---------------+----------------+-----------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fulldataset = new_crimes.join(new_census, on='CTN',how='left')\n",
    "fulldataset.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset and calculate baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df, testing_df = fulldataset.randomSplit([0.6, 0.3, 0.1],seed=0)\n",
    "\n",
    "count_theft=training_df.filter(fn.col('CODE_DEFINED')==\"Theft\").count()\n",
    "count_property=training_df.filter(fn.col('CODE_DEFINED')==\"Property\").count()\n",
    "count_drug=training_df.filter(fn.col('CODE_DEFINED')==\"Drug\").count()\n",
    "count_assault=training_df.filter(fn.col('CODE_DEFINED')==\"ASSAULT\").count()\n",
    "count_all=training_df.count()#+validation_df.count()\n",
    "baseline=max(count_theft,count_property,count_drug,count_assault)/count_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5838560581049852"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+-----+---+---+---+---+---+---+---+---+---+---+---+---------+-------+------------+----------+--------------+------------+--------+-------+-------+--------------+--------------------+----------+------------+----------------------+-----------+--------------+---------------+----------------+-----------------------+-------------------+\n",
      "|CTN|              DATE1|     DATE2|Month|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|TIMESTART|TIMEEND|CODE_DEFINED|index_code|TIMESTART_MINS|TIMEEND_MINS|Midnight|Morning|Daytime|            CT|           Long name|Population|Square miles|people per square mile|Tenure Rate|Occupancy Rate|Low Income Rate|High Income Rate|Expensive Property Rate|High Education Rate|\n",
      "+---+-------------------+----------+-----+---+---+---+---+---+---+---+---+---+---+---+---------+-------+------------+----------+--------------+------------+--------+-------+-------+--------------+--------------------+----------+------------+----------------------+-----------+--------------+---------------+----------------+-----------------------+-------------------+\n",
      "|100|2016-05-13 04:00:00| 5/13/2016|  May|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|     2120|   2120|       Theft|         1|          1280|        1280|       0|      0|      0|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2016-06-05 04:00:00|  6/5/2016|  Jun|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|     1840|   1840|       Theft|         1|          1120|        1120|       0|      0|      0|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2016-12-23 05:00:00|12/23/2016|  Dec|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|      851|    851|       Theft|         1|           531|         531|       0|      1|      0|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2017-01-01 05:00:00|  1/1/2017|  Jan|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     1928|   1928|       Theft|         1|          1168|        1168|       0|      0|      0|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "|100|2017-01-03 05:00:00|  1/3/2017|  Jan|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|      830|   1415|       Theft|         1|           510|         855|       0|      0|      1|Census Tract 1|Census Tract 1,On...|       674|         1.9|                 363.5|0.875536481|   0.894433781|    0.362660944|     0.216738197|            0.103448276|        0.598566308|\n",
      "+---+-------------------+----------+-----+---+---+---+---+---+---+---+---+---+---+---+---------+-------+------------+----------+--------------+------------+--------+-------+-------+--------------+--------------------+----------+------------+----------------------+-----------+--------------+---------------+----------------+-----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.select(fn.col(\"*\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning logistic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "va=feature.VectorAssembler(inputCols=['Feb',\n",
    "                                      'Mar',\n",
    "                                      'Apr',\n",
    "                                      'May',\n",
    "                                      'Jun',\n",
    "                                      'Jul',\n",
    "                                      'Aug',\n",
    "                                      'Sep',\n",
    "                                      'Oct',\n",
    "                                      'Nov',\n",
    "                                      'Dec',\n",
    "                                      'Midnight',\n",
    "                                      'Morning',\n",
    "                                      'Daytime',\n",
    "                                      \"Population\",\n",
    "                                      \"Square miles\",\n",
    "                                      \"people per square mile\",\n",
    "                                      \"Tenure Rate\",\n",
    "                                      \"Occupancy Rate\",\n",
    "                                      \"Low Income Rate\",\n",
    "                                      \"High Income Rate\",\n",
    "                                      \"Expensive Property Rate\",\n",
    "                                      \"High Education Rate\"\n",
    "                                      ],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy = 0.6381404447246997\n",
      "Baseline Accuracy = 0.5838560581049852\n"
     ]
    }
   ],
   "source": [
    "#labelIndexer = StringIndexer(inputCol=\"CODE_DEFINED\", outputCol=\"indexedCODE_DEFINED\").fit(training_df)\n",
    "#featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=100).fit(training_df)\n",
    "maxacu=0\n",
    "maxdep=0\n",
    "for i in range(1,15):\n",
    "    DT_model=classification.DecisionTreeClassifier(maxDepth=i,labelCol='index_code',featuresCol='features')\n",
    "    DT_tr=Pipeline(stages=[va,DT_model]).fit(training_df)\n",
    "    result_DT_tr=DT_tr.transform(training_df).select(\"index_code\",\"prediction\")\n",
    "    evaluator = MulticlassClassificationEvaluator().setLabelCol(\"index_code\").setPredictionCol(\"prediction\")\n",
    "    tr_DT_Accuracy = evaluator.evaluate(result_DT_tr)\n",
    "    result_DT_vl=DT_tr.transform(validation_df).select('index_code','prediction')\n",
    "    evaluator = MulticlassClassificationEvaluator().setLabelCol(\"index_code\").setPredictionCol(\"prediction\")\n",
    "    vl_DT_Accuracy = evaluator.evaluate(result_DT_vl)\n",
    "    if vl_DT_Accuracy>maxacu:\n",
    "        maxacu=vl_DT_Accuracy\n",
    "        maxdep=i\n",
    "#print(\"Training Accuracy = \" + str(tr_DT_Accuracy))\n",
    "print(\"Validation Accuracy = \" + str(maxacu))\n",
    "print('Baseline Accuracy = ' + str(baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Accuracy = \" + str(maxacu))\n",
    "print('Baseline Accuracy = ' + str(baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
